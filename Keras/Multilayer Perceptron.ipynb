{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multi-layer Perceptron in Keras</h2>\n",
    "<p>Train a simple deep NN on the MNIST dataset. Get to 98.40% test accuracy after 20 epochs (there is *a lot* of margin for parameter tuning). 2 seconds per epoch on a K520 GPU.</p>\n",
    "\n",
    "<p>\n",
    "For this tutorial, a multilayer perceptron (MLP) is built using Keras. It is trained and tested using the MNIST handwritten digits dataset. The MLP consists of two hidden, fully connected layers, and an output layer using softmax to determine probability of each class (0-9).\n",
    "</p>\n",
    "<p>\n",
    "This example is pulled from the examples on the Keras Github repository. It has been converted into an iPython notebook for ease of use and shareability. It has also been modified slightly to explore flexibility in hyperparameters.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size for stochastic gradient descent; e.g. number of samples per run\n",
    "batch_size = 128\n",
    "# Output number of classes. MNIST has 10 possible classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "nb_classes = 10\n",
    "# Number of iterations over the entire dataset when training\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Reshape the datasets, flatten each image as a single dimensional vector\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize the training set to a value between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer, Dense is fully connected, input is vector of size 784, and number of hidden nodes is 512\n",
    "# The number of hidden nodes is a hyperparameter to explore when testing various models\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "# For each node, sum the input x weights, and run Rectified Linear Unit (ReLu) activation function. Can also use\n",
    "# tanh, sigmoid, softplus, relu, hard_sigmoid, linear. The softmax activation is also available, but only makes sense\n",
    "# to use this activation for output, as this is probability of classification.\n",
    "model.add(Activation('relu'))\n",
    "# Dropout is used as a percentage of inputs to exclude during backpropagation, gradient updates. Here, 20% of the\n",
    "# input units are \"dropped\" and not updated during backprop. This is to help prevent overfitting.\n",
    "model.add(Dropout(0.2))\n",
    "# A second hidden layer, with the 512 outputs of the first hidden layer as the input to this layer. Also has 512 nodes\n",
    "model.add(Dense(512))\n",
    "# Activation function for hidden layer 2\n",
    "model.add(Activation('relu'))\n",
    "# Dropout percentage for hidden layer 2\n",
    "model.add(Dropout(0.2))\n",
    "# Output layer, fully connected to 10 nodes, for each possible class (0-9)\n",
    "model.add(Dense(nb_classes))\n",
    "# Softmax is an activation function that converts the values to a probability for that particular class. \n",
    "# A generalization of the logistic function \n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model, using the RMSprop optimizer, and a the categorical cross entropy loss function.\n",
    "\n",
    "# RMSprop is a variant of stochastic gradient descent. Uses a mini-batch processing and keeps a running \n",
    "# average of previous gradients to normalize the gradients\n",
    "# Many additional optimizers available, including ability to build/write custom optimizers\n",
    "\n",
    "# Categorical_crossentropy is used with softmax to determine the N-category cross entropy of the predicted vs. \n",
    "# target variable category. Also known as multiclass logloss.\n",
    "# Many additional loss functions are available, including mean_squared_error / mse, root_mean_squared_error / rmse\n",
    "#mean_absolute_error / mae, mean_absolute_percentage_error / mape, mean_squared_logarithmic_error / msle, squared_hinge\n",
    "# hinge, binary_crossentropy: Also known as logloss., categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.2742 - acc: 0.9164 - val_loss: 0.1133 - val_acc: 0.9647\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.1135 - acc: 0.9660 - val_loss: 0.0875 - val_acc: 0.9729\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 71s - loss: 0.0798 - acc: 0.9754 - val_loss: 0.0805 - val_acc: 0.9754\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.0627 - acc: 0.9805 - val_loss: 0.0708 - val_acc: 0.9773\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.0514 - acc: 0.9836 - val_loss: 0.0706 - val_acc: 0.9785\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 71s - loss: 0.0409 - acc: 0.9873 - val_loss: 0.0560 - val_acc: 0.9833\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 70s - loss: 0.0336 - acc: 0.9890 - val_loss: 0.0659 - val_acc: 0.9812\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 70s - loss: 0.0300 - acc: 0.9902 - val_loss: 0.0570 - val_acc: 0.9848\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0614 - val_acc: 0.9836\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 71s - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0634 - val_acc: 0.9819\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0607 - val_acc: 0.9835\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 70s - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0672 - val_acc: 0.9832\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 70s - loss: 0.0167 - acc: 0.9940 - val_loss: 0.0600 - val_acc: 0.9850\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 71s - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0672 - val_acc: 0.9837\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 70s - loss: 0.0130 - acc: 0.9956 - val_loss: 0.0647 - val_acc: 0.9844\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.0127 - acc: 0.9954 - val_loss: 0.0699 - val_acc: 0.9831\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 71s - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0770 - val_acc: 0.9833\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 71s - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0737 - val_acc: 0.9847\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 73s - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0684 - val_acc: 0.9854\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 72s - loss: 0.0073 - acc: 0.9975 - val_loss: 0.0728 - val_acc: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa386a60c90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Begin Training the model\n",
    "#\n",
    "# Pass the training set: input and targets\n",
    "# batch_size: size of the mini batch, or number of samples to run at once, including gradient updates, \n",
    "# rather than run the entire dataset\n",
    "# nb_epoch: number of epochs or iterations over the entire dataset\n",
    "# show_accuracy: whether or not to display the accuracy for each epoch while training\n",
    "# verbose: how much detail to display, 0 - No output, 1 - More detail, 2 - Less detail\n",
    "# validation_data: Dataset the model is validated against, the output displays the loss and accuracy \n",
    "# of the validation set\n",
    "#\n",
    "# The loss function should be minimized. Accuracy is a percentage, e.g. ~1.0 yields 100% accuracy\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the trained model on the test set. For this example, the test and validation sets are the same. This function\n",
    "# is useful for running the model on a new dataset not previously seen. \n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0727956595342\n",
      "Test accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "# print the categorical_crossentropy value of model run on the test set\n",
    "print('Test score:', score[0])\n",
    "# print the accuracy of the model run on the test set\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
