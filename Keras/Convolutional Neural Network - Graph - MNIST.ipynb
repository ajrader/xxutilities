{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convolutional Neural Network with Graph in Keras</h2>\n",
    "<p>Train a simple convnet on the MNIST dataset.</p>\n",
    "<p>Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n",
    "Get to 99.25% test accuracy after 12 epochs (there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU</p>\n",
    "<p>\n",
    "For this tutorial, a convolutional neural network (CNN) is built using Keras. It is trained and tested using the MNIST handwritten digits dataset. The CNN consists of multiple layers of convolution and max pooling, ending with a fully connected MLP for classification. \n",
    "</p>\n",
    "<p>\n",
    "This example is built using the Graph model rather than a Sequential model. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337) # for reproducibility of initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size for stochastic gradient descent; e.g. number of samples per run\n",
    "batch_size = 128\n",
    "# Output number of classes. MNIST has 10 possible classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "nb_classes = 10\n",
    "# Number of iterations over the entire dataset when training\n",
    "nb_epoch = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions MNIST\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of image bands, RGB, or single band\n",
    "nb_image_bands = 1\n",
    "# number of convolutional filters to use, can be different for multiple convolutional layers\n",
    "nb_filters1 = 32\n",
    "nb_filters2 = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size , can vary for different layers\n",
    "nb_conv1 = 3 # (3x3 covolution)\n",
    "nb_conv2 = 4 # (4x4 covolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between tran and test sets, may have issues with proxy/firewall here. \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 1, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Reshape to include a 4th dimension, such that the dataset is tupled as (num_samples, num_bands, img_num_rows, img_num_cols)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize the training set to a value between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize an empty Graph model\n",
    "model = Graph()\n",
    "\n",
    "model.add_input(name='input', input_shape=(nb_image_bands, img_rows, img_cols))\n",
    "# First layer: Convolution2D - Generates 32 feature maps, using a 3x3 convolution filter\n",
    "model.add_node(Convolution2D(nb_filters1, nb_conv1, nb_conv1, border_mode='valid'), name='conv1', input='input')\n",
    "# For each node, sum the input x weights, and run Rectified Linear Unit (ReLu) activation function. Can also use\n",
    "# tanh, sigmoid, softplus, relu, hard_sigmoid, linear. The softmax activation is also available, but only makes sense\n",
    "# to use this activation for output, as this is probability of classification.\n",
    "model.add_node(Activation('relu'), name='activation1', input='conv1')\n",
    "# Run max pooling using a 2x2 pooling filter, storing the maximum value\n",
    "model.add_node(MaxPooling2D(pool_size=(nb_pool, nb_pool)), name='maxpool1', input='activation1')\n",
    "# Second Convolution layer - Generates 32 feature maps using a 4x4 convolution filter\n",
    "model.add_node(Convolution2D(nb_filters2, nb_conv2, nb_conv2, border_mode='valid'), name='conv2', input='maxpool1')\n",
    "# Activation for second convolution step\n",
    "model.add_node(Activation('relu'), name='activation2', input='conv2')\n",
    "# Second max pooling step\n",
    "model.add_node(MaxPooling2D(pool_size=(nb_pool, nb_pool)), name='maxpool2', input='activation2')\n",
    "# Dropout is used as a percentage of inputs to exclude during backpropagation, gradient updates. Here, 20% of the\n",
    "# input units are \"dropped\" and not updated during backprop. This is to help prevent overfitting.\n",
    "model.add_node(Dropout(0.25), name='dropout1', input='maxpool2')\n",
    "\n",
    "# After convolutions/max pooling, the features extracted can then be passed through a classification algorithm.\n",
    "# A common approach is to simply the features through a fully connected layer before classifying using softmax.\n",
    "\n",
    "# Convert the features into a single dimension vector\n",
    "model.add_node(Flatten(), name='flatten', input='dropout1')\n",
    "# Add a fully connected hidden layer of 128 nodes - This can be modified as a hyperparameter for testing various models\n",
    "model.add_node(Dense(128), name='hidden1', input='flatten')\n",
    "# ReLu activation function for hidden layer\n",
    "model.add_node(Activation('relu'), name='activation3', input='hidden1')\n",
    "# Dropout percentage for hidden layer\n",
    "model.add_node(Dropout(0.5), name='dropout2', input='activation3')\n",
    "# Output layer, fully connected to 10 nodes, for each possible class (0-9)\n",
    "model.add_node(Dense(nb_classes), name='output', input='dropout2')\n",
    "# Softmax is an activation function that converts the values to a probability for that particular class. \n",
    "# A generalization of the logistic function \n",
    "model.add_node(Activation('softmax'), name='softmax', input='output')\n",
    "# Add model output\n",
    "model.add_output(name='outputActivation', input='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.76 s, sys: 23 ms, total: 2.78 s\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "# Compile the model, using the RMSprop optimizer, and a the categorical cross entropy loss function.\n",
    "\n",
    "# ADADelta is a variant of stochastic gradient descent. A per-dimension learning-rate method that adapts over time, \n",
    "# requires no manual parameter tuning\n",
    "\n",
    "# Categorical_crossentropy is used with softmax to determine the N-category cross entropy of the predicted vs. \n",
    "# target variable category. Also known as multiclass logloss.\n",
    "# Many additional loss functions are available, including mean_squared_error / mse, root_mean_squared_error / rmse\n",
    "# mean_absolute_error / mae, mean_absolute_percentage_error / mape, mean_squared_logarithmic_error / msle, squared_hinge\n",
    "# hinge, binary_crossentropy: Also known as logloss., categorical_crossentropy\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "%time model.compile(optimizer = 'adadelta', loss = {'outputActivation':'categorical_crossentropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 451s - loss: 0.1380 - val_loss: 0.0417\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 450s - loss: 0.0842 - val_loss: 0.0325\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 454s - loss: 0.0678 - val_loss: 0.0430\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 448s - loss: 0.0590 - val_loss: 0.0295\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 447s - loss: 0.0523 - val_loss: 0.0265\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 450s - loss: 0.0479 - val_loss: 0.0241\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 463s - loss: 0.0431 - val_loss: 0.0248\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 463s - loss: 0.0410 - val_loss: 0.0233\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 467s - loss: 0.0380 - val_loss: 0.0236\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 457s - loss: 0.0366 - val_loss: 0.0245\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 459s - loss: 0.0340 - val_loss: 0.0213\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 460s - loss: 0.0317 - val_loss: 0.0236\n",
      "CPU times: user 1h 31min 1s, sys: 9.39 s, total: 1h 31min 11s\n",
      "Wall time: 1h 31min 15s\n"
     ]
    }
   ],
   "source": [
    "# Begin Training the model\n",
    "#\n",
    "# Pass the training set: input and targets\n",
    "# batch_size: size of the mini batch, or number of samples to run at once, including gradient updates, \n",
    "# rather than run the entire dataset\n",
    "# nb_epoch: number of epochs or iterations over the entire dataset\n",
    "# verbose: how much detail to display, 0 - No output, 1 - More detail, 2 - Less detail\n",
    "# validation_data: Dataset the model is validated against, the output displays the loss and accuracy \n",
    "# of the validation set\n",
    "#\n",
    "# The loss function should be minimized. Note that the graph model does not have a show_accuracy. According to the\n",
    "# developers, due to the complexity of the graph model, it is very difficult to include this value as output of the\n",
    "# model. Their recommendation is that it is much easier to take the predictions and calculate the accuracy directly.\n",
    "%time history = model.fit({'input':X_train, 'outputActivation':Y_train}, nb_epoch=nb_epoch, \\\n",
    "                    batch_size=batch_size, verbose=1, validation_data=({'input':X_test, 'outputActivation':Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 14s    \n"
     ]
    }
   ],
   "source": [
    "# Run the trained model on the test set. For this example, the test and validation sets are the same. This function\n",
    "# is useful for running the model on a new dataset not previously seen. \n",
    "score = model.evaluate({'input': X_test, 'outputActivation': Y_test}, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 14s    \n"
     ]
    }
   ],
   "source": [
    "# Graph model does not have an accuracy. Here, we calculate it outselves\n",
    "prediction = model.predict({'input': X_test}, batch_size=batch_size, verbose=1)\n",
    "#Calculate the abs of the differences between the predicted value and the target value. Sum all the errors, divided by\n",
    "# number of samples to get the percent of error. Accuracy is 1 - percent error.\n",
    "accuracy = 1 - np.sum(np.abs(prediction['outputActivation'] - Y_test)) / len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0235688971492\n",
      "Test accuracy: 0.980328007724\n"
     ]
    }
   ],
   "source": [
    "# print the categorical_crossentropy value of model run on the test set\n",
    "print('Test score:', score)\n",
    "# print the accuracy of the model run on the test set\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
