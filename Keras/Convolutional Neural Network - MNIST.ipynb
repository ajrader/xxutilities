{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convolutional Neural Network in Keras</h2>\n",
    "<p>Train a simple convnet on the MNIST dataset.</p>\n",
    "<p>Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n",
    "Get to 99.25% test accuracy after 12 epochs (there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU</p>\n",
    "<p>\n",
    "For this tutorial, a convolutional neural network (CNN) is built using Keras. It is trained and tested using the MNIST handwritten digits dataset. The CNN consists of multiple layers of convolution and max pooling, ending with a fully connected MLP for classification. \n",
    "</p>\n",
    "<p>\n",
    "This example is pulled from the examples on the Keras Github repository. It has been converted into an iPython notebook for ease of use and shareability. It has also been modified slightly to explore flexibility in hyperparameters.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337) # for reproducibility of initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size for stochastic gradient descent; e.g. number of samples per run\n",
    "batch_size = 128\n",
    "# Output number of classes. MNIST has 10 possible classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "nb_classes = 10\n",
    "# Number of iterations over the entire dataset when training\n",
    "nb_epoch = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions MNIST\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use, can be different for multiple convolutional layers\n",
    "nb_filters1 = 32\n",
    "nb_filters2 = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size , can vary for different layers\n",
    "nb_conv1 = 3 # (3x3 covolution)\n",
    "nb_conv2 = 4 # (4x4 covolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between tran and test sets, may have issues with proxy/firewall here. \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 1, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Reshape to include a 4th dimension, such that the dataset is tupled as (num_samples, num_bands, img_num_rows, img_num_cols)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize the training set to a value between 0 and 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize an empty model\n",
    "model = Sequential()\n",
    "\n",
    "# First layer: Convolution2D - Generates 32 feature maps, using a 3x3 convolution filter\n",
    "model.add(Convolution2D(nb_filters1, nb_conv1, nb_conv1, border_mode='valid', input_shape=(1, img_rows, img_cols)))\n",
    "# For each node, sum the input x weights, and run Rectified Linear Unit (ReLu) activation function. Can also use\n",
    "# tanh, sigmoid, softplus, relu, hard_sigmoid, linear. The softmax activation is also available, but only makes sense\n",
    "# to use this activation for output, as this is probability of classification.\n",
    "model.add(Activation('relu'))\n",
    "# Run max pooling using a 2x2 pooling filter, storing the maximum value\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "# Second Convolution layer - Generates 32 feature maps using a 4x4 convolution filter\n",
    "model.add(Convolution2D(nb_filters2, nb_conv2, nb_conv2))\n",
    "# Activation for second convolution step\n",
    "model.add(Activation('relu'))\n",
    "# Second max pooling step\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "# Dropout is used as a percentage of inputs to exclude during backpropagation, gradient updates. Here, 25% of the\n",
    "# input units are \"dropped\" and not updated during backprop. This is to help prevent overfitting.\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# After convolutions/max pooling, the features extracted can then be passed through a classification algorithm.\n",
    "# A common approach is to simply the features through a fully connected layer before classifying using softmax.\n",
    "\n",
    "# Convert the features into a single dimension vector\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a fully connected hidden layer of 128 nodes - This can be modified as a hyperparameter for testing various models\n",
    "model.add(Dense(128))\n",
    "# ReLu activation function for hidden layer\n",
    "model.add(Activation('relu'))\n",
    "# Dropout percentage for hidden layer\n",
    "model.add(Dropout(0.5))\n",
    "# Output layer, fully connected to 10 nodes, for each possible class (0-9)\n",
    "model.add(Dense(nb_classes))\n",
    "# Softmax is an activation function that converts the values to a probability for that particular class. \n",
    "# A generalization of the logistic function \n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile the model, using the RMSprop optimizer, and a the categorical cross entropy loss function.\n",
    "\n",
    "# ADADelta is a variant of stochastic gradient descent. A per-dimension learning-rate method that adapts over time, \n",
    "# requires no manual parameter tuning\n",
    "\n",
    "# Categorical_crossentropy is used with softmax to determine the N-category cross entropy of the predicted vs. \n",
    "# target variable category. Also known as multiclass logloss.\n",
    "# Many additional loss functions are available, including mean_squared_error / mse, root_mean_squared_error / rmse\n",
    "# mean_absolute_error / mae, mean_absolute_percentage_error / mape, mean_squared_logarithmic_error / msle, squared_hinge\n",
    "# hinge, binary_crossentropy: Also known as logloss., categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "447s - loss: 0.2541 - acc: 0.9205 - val_loss: 0.0531 - val_acc: 0.9822\n",
      "Epoch 2/12\n",
      "454s - loss: 0.0973 - acc: 0.9702 - val_loss: 0.0414 - val_acc: 0.9869\n",
      "Epoch 3/12\n",
      "447s - loss: 0.0706 - acc: 0.9790 - val_loss: 0.0322 - val_acc: 0.9895\n",
      "Epoch 4/12\n",
      "446s - loss: 0.0598 - acc: 0.9817 - val_loss: 0.0311 - val_acc: 0.9892\n",
      "Epoch 5/12\n",
      "446s - loss: 0.0553 - acc: 0.9835 - val_loss: 0.0285 - val_acc: 0.9903\n",
      "Epoch 6/12\n",
      "455s - loss: 0.0504 - acc: 0.9845 - val_loss: 0.0256 - val_acc: 0.9905\n",
      "Epoch 7/12\n",
      "466s - loss: 0.0463 - acc: 0.9870 - val_loss: 0.0230 - val_acc: 0.9912\n",
      "Epoch 8/12\n",
      "459s - loss: 0.0423 - acc: 0.9870 - val_loss: 0.0223 - val_acc: 0.9921\n",
      "Epoch 9/12\n",
      "462s - loss: 0.0380 - acc: 0.9881 - val_loss: 0.0253 - val_acc: 0.9917\n",
      "Epoch 10/12\n",
      "458s - loss: 0.0364 - acc: 0.9893 - val_loss: 0.0225 - val_acc: 0.9932\n",
      "Epoch 11/12\n",
      "457s - loss: 0.0361 - acc: 0.9894 - val_loss: 0.0209 - val_acc: 0.9928\n",
      "Epoch 12/12\n",
      "457s - loss: 0.0328 - acc: 0.9897 - val_loss: 0.0229 - val_acc: 0.9927\n",
      "CPU times: user 1h 30min 52s, sys: 4.93 s, total: 1h 30min 57s\n",
      "Wall time: 1h 30min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36b3086250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Begin Training the model\n",
    "#\n",
    "# Pass the training set: input and targets\n",
    "# batch_size: size of the mini batch, or number of samples to run at once, including gradient updates, \n",
    "# rather than run the entire dataset\n",
    "# nb_epoch: number of epochs or iterations over the entire dataset\n",
    "# show_accuracy: whether or not to display the accuracy for each epoch while training\n",
    "# verbose: how much detail to display, 0 - No output, 1 - More detail, 2 - Less detail\n",
    "# validation_data: Dataset the model is validated against, the output displays the loss and accuracy \n",
    "# of the validation set\n",
    "#\n",
    "# The loss function should be minimized. Accuracy is a percentage, e.g. ~1.0 yields 100% accuracy\n",
    "%time model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, \\\n",
    "                verbose=2, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the trained model on the test set. For this example, the test and validation sets are the same. This function\n",
    "# is useful for running the model on a new dataset not previously seen. \n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.0229262540997\n",
      "Test accuracy: 0.9927\n"
     ]
    }
   ],
   "source": [
    "# print the categorical_crossentropy value of model run on the test set\n",
    "print('Test score:', score[0])\n",
    "# print the accuracy of the model run on the test set\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
